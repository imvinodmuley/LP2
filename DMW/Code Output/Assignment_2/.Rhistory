cut_avg <- cutree(res.hc, k = 3)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
rect.hclust(res.hc , k = 3, border = 2:6)
abline(h = 3, col = 'red')
#Hierarchical clustering
require(stats)
require(factoextra)
require(ggplot2)
require(stats)
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:6,1:6]
x
round(x,digits = 3);
res.hc <- hclust(d=res.dist, method = "complete");
#cut_avg <- cutree(res.hc, k = 3)
plot(x=res.hc)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
rect.hclust(res.hc , k = 3, border = 2:6)
abline(h = 3, col = 'red')
plot(x=res.hc)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
require(stats)
require(factoextra)
require(ggplot2)
require(stats)
#Scaling the dataset
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 4 groups
sub_grp <- cutree(res.hc, k = 3)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_cluster(list(data = res.hc, cluster = sub_grp))
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:7)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 5:5)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
plot(res.hc, cex = 0.2)
rect.hclust(res.hc, k = 3, border = 2:5)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
#K-means clustering
#read data from file to dataframe
df1<-read.table("sub.data",header=TRUE,sep=",");
View(df1);
#Implementing k-means clustering algorithm
results <- kmeans(df1, 3);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
results <- kmeans(df1, 2);
results
#K-means clustering
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
wssplot <- function(Iris.features, nc=15, seed=1234){
wss <- (nrow(Iris.features)-1)*sum(apply(Iris.features,2,var))
for(i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(Iris.features, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab = "Number Of clusters",
ylab = "Within groups sum of squares")
}
#Implementing k-means clustering algorithm
results <- kmeans(Iris.features, 3);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
wssplot <- function(Iris.features, nc=15, seed=1234){
wss <- (nrow(Iris.features)-1)*sum(apply(Iris.features,2,var))
for(i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(Iris.features, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab = "Number Of clusters",
ylab = "Within groups sum of squares")
}
View(wssplot)
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for(i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab = "Number Of clusters",
ylab = "Within groups sum of squares")
}
wssplot(Iris.features)
require(stats)
require(factoextra)
require(ggplot2)
require(stats)
results <- kmeans(Iris.features, 2);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
##
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
results$size
results$centers
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
wssplot(Iris.features)
#Implementing k-means clustering algorithm
results <- kmeans(Iris.features, 3);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
##
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
wssplot(Iris.features)
#Implementing k-means clustering algorithm
results <- kmeans(Iris.features, 3);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
##
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
wssplot(Iris.features)
#Implementing k-means clustering algorithm
results <- kmeans(Iris.features, 2);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
##
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 3 groups
sub_grp <- cutree(res.hc, k = 3)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
sub_grp <- cutree(res.hc, k = 2)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
sub_grp <- cutree(res.hc, k = 2)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 2, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
#K-means clustering
#Packages
require(stats)
require(factoextra)
require(ggplot2)
require(stats)
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
#wss plot to choose max num of cluster
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for(i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab = "Number Of clusters",
ylab = "Within groups sum of squares")
}
wssplot(Iris.features)
#Implementing k-means clustering algorithm
results <- kmeans(Iris.features, 2);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
##
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
#Hierarchical clustering
#Scaling the dataset
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 3 groups
sub_grp <- cutree(res.hc, k = 3)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
results
results$size
results$centers
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
x<- as.matrix(res.dist)[1:5,1:5]
x
res.hc <- hclust(d=res.dist, method = "complete");
plot(res.hc, cex = 0.6)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
#Hierarchical clustering
#K-means clustering
#Packages
require(stats)
require(factoextra)
require(ggplot2)
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
#Scaling the dataset
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 3 groups
sub_grp <- cutree(res.hc, k = 3)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
plot(res.hc, cex = 0.6)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
rect.hclust(res.hc, k = 3, border = 2:5)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
sub_grp <- cutree(res.hc, k = 3)
fviz_dend(x=res.hc,cex=0.5,lw=0.2)
#Hierarchical clustering
#K-means clustering
#Packages
require(stats)
require(factoextra)
require(ggplot2)
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
#Scaling the dataset
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 3 groups
#sub_grp <- cutree(res.hc, k = 3)
# fviz_dend(x=res.hc,cex=0.5,lw=0.2)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
#Hierarchical clustering
#Packages
require(stats)
require(factoextra)
require(ggplot2)
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
#Scaling the dataset
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 3 groups
sub_grp <- cutree(res.hc, k = 3);
# fviz_dend(x=res.hc,cex=0.5,lw=0.2)
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
fviz_cluster(list(data = Iris.features, cluster = sub_grp));
#K-means clustering
#Packages
require(stats)
#K-means clustering
#Packages
require(stats)
require(factoextra)
require(ggplot2)
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
#wss plot to choose max num of cluster
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for(i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab = "Number Of clusters",
ylab = "Within groups sum of squares")
}
wssplot(Iris.features)
#Implementing k-means clustering algorithm
results <- kmeans(Iris.features, 2);
results
#component wise results
results$size
results$centers
#how the data divided into clusters
table(Iris$species, results$cluster);
#Plotting the graph
plot(Iris[c("sepal_length","sepal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("sepal_length","sepal_width")], col = iris$Species);
plot(Iris[c("petal_length","petal_width")], col = results$cluster);
#comparing plot with original dataset
plot(Iris[c("petal_length","petal_width")], col = iris$Species);
#Graph
fviz_cluster(list(data = Iris.features, cluster = results$cluster));
#Hierarchical clustering
#Packages
require(stats)
require(factoextra)
require(ggplot2)
#read data from file to dataframe
Iris<-read.table("iris.data",header=TRUE,sep=",");
View(Iris);
# Create another dataframe for operations
Iris.features <- Iris;
Iris.features$species <- NULL;
View(Iris.features);
#Scaling the dataset
Iris.scaled <-scale(Iris.features) #mean 0 sd 1
head(Iris.scaled)
#Computing distance between observations return distance matrix
res.dist <- dist(x=Iris.scaled, method = "euclidean" )
x<- as.matrix(res.dist)[1:5,1:5]
x
round(x,digits = 3);
#Implementing Hierarchical clustering
#Linkage methods
#Complete-linkage : computes max distance before merging
#Single-linkage : computes min distance between clusters before merging
#Average-linkage : computes avg distance before merging.
#Centroid-linkage : computes distance between centroids before merging
res.hc <- hclust(d=res.dist, method = "complete");
# Cut tree into 3 groups and plot ClusterPlot
sub_grp <- cutree(res.hc, k = 3);
fviz_cluster(list(data = Iris.features, cluster = sub_grp));
#plot Dendogram with k = 3 clusters
plot(res.hc, cex = 0.6)
rect.hclust(res.hc, k = 3, border = 2:5)
